{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing with Neural Network\n",
    "## Lecture 16: Simple RNN\n",
    "\n",
    "##  Weather Data\n",
    "\n",
    "<img src='../../../images/prasami_color_tutorials_small.png' style = 'width:400px;' alt=\"By Pramod Sharma : pramod.sharma@prasami.com\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iAve6DCL4JH4"
   },
   "outputs": [],
   "source": [
    "###-----------------\n",
    "\n",
    "# Import Libraries\n",
    "\n",
    "###-----------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#from utils.helper import fn_plot_tf_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###----------------------\n",
    "### Some basic parameters\n",
    "###----------------------\n",
    "\n",
    "\n",
    "inpDir = 'C:/Users/Rohit/OneDrive/Desktop/DNN_Assign/input' # location where input data is stored\n",
    "outDir = '../output' # location to store outputs\n",
    "modelDir = '../models'\n",
    "subDir = 'flower_photos'\n",
    "valDir = 'valid_flowers'\n",
    "altName = 'cnn_base'\n",
    "\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "tf.random.set_seed(RANDOM_STATE) # setting for Tensorflow as well\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "ALPHA = 0.001\n",
    "EPOCHS = 1 # number of cycles to run\n",
    "FLIP_MODE = 'horizontal_and_vertical'\n",
    "ROTATION=(-0.1,0.1)\n",
    "BATCH_SIZE = 8 \n",
    "IMG_HEIGHT = 200 \n",
    "IMG_WIDTH = 200 \n",
    "\n",
    "\n",
    "# Set parameters for decoration of plots\n",
    "params = {'legend.fontsize' : 'medium',\n",
    "          'figure.figsize'  : (15,10),\n",
    "          'axes.labelsize'  : 'medium',\n",
    "          'axes.titlesize'  :'large',\n",
    "          'xtick.labelsize' :'medium',\n",
    "          'ytick.labelsize' :'medium',\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.coolwarm\n",
    "\n",
    "plt.rcParams.update(params) # update rcParams\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# verify if the directory exists\n",
    "def fn_dir_exists(_dir):\n",
    "    '''\n",
    "    Args:\n",
    "    \t_dir: path if a directory\n",
    "     '''\n",
    "\n",
    "    res = os.path.exists(_dir)\n",
    "\n",
    "    fn_log_event ('-- Directory \"{}\" exist : {}'.format(_dir, res), 'debug')\n",
    "\n",
    "    if not res:\n",
    "\n",
    "        fn_log_event ('-- Directory \"{}\" does not exists.'.format(_dir), 'debug')\n",
    "\n",
    "        sys.exit('-- Directory \"{}\" does not exists.'.format(_dir))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def fn_file_exists(_file):\n",
    "    '''\n",
    "        verify if the file exists\n",
    "    '''\n",
    "    res = os.path.exists(_file)\n",
    "\n",
    "    fn_log_event ('-- File \"{}\" exist : {}'.format(_file, res), 'debug')\n",
    "\n",
    "    if not res:\n",
    "\n",
    "        fn_log_event ('-- File \"{}\" does not exists.'.format(_file), 'debug')\n",
    "\n",
    "        sys.exit('-- File \"{}\" does not exists.'.format(_file))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "###-----------------------------\n",
    "### Verify or create a directory\n",
    "###-----------------------------\n",
    "def fn_verify_dir(_path : str):\n",
    "    '''\n",
    "    Arg:\n",
    "        path: path to verify the directory\n",
    "    returns:\n",
    "        create dir if it does not exists\n",
    "    '''\n",
    "    if os.path.exists(_path): # check if the path exists. Maybe a file or a folder\n",
    "\n",
    "        print(_path, ' exists') # advised the user\n",
    "\n",
    "    else:\n",
    "\n",
    "        os.makedirs(_path) # create the path\n",
    "\n",
    "        print(\"Created folder : \", _path)\n",
    "\n",
    "###------------------------------------------------\n",
    "### Plot Loss Curve using Tensorflow history object\n",
    "###------------------------------------------------\n",
    "def fn_plot_tf_hist(hist_df):\n",
    "\n",
    "    '''\n",
    "    Args:\n",
    "        hist_df: a dataframe with following ccolumns\n",
    "            column 0: loss\n",
    "            column 1: accuracy\n",
    "            column 2: val_loss\n",
    "            column 3: val_accuracy\n",
    "            While plotting columns are accessed by index\n",
    "            so that even if the column names are different it will not throw exceptions.\n",
    "    '''\n",
    "\n",
    "    fig, axes = plt.subplots(1,2 , figsize = (15,6))\n",
    "\n",
    "    # properties  matplotlib.patch.Patch\n",
    "    props = dict(boxstyle='round', facecolor='aqua', alpha=0.4)\n",
    "    facecolor = 'cyan'\n",
    "    fontsize=12\n",
    "    CMAP = plt.cm.coolwarm\n",
    "\n",
    "    # Get columns by index to eliminate any column naming error\n",
    "    y1 = hist_df.columns[0]\n",
    "    y2 = hist_df.columns[1]\n",
    "    y3 = hist_df.columns[2]\n",
    "    y4 = hist_df.columns[3]\n",
    "\n",
    "    # Where was min loss\n",
    "    best = hist_df[hist_df[y3] == hist_df[y3].min()]\n",
    "\n",
    "    ax = axes[0]\n",
    "\n",
    "    hist_df.plot(y = [y1,y3], ax = ax, colormap=CMAP)\n",
    "\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = \"Loss: \\n  train: {:6.4f}\\n   test: {:6.4f}\"\n",
    "    txtstr = txtFmt.format(hist_df.iloc[-1][y1],\n",
    "                           hist_df.iloc[-1][y3]) #text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.95, txtstr, transform=ax.transAxes, fontsize=fontsize,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # calculate offset for arroe\n",
    "    y_min = min(hist_df[y1].min(), hist_df[y3].min())\n",
    "    y_max = max(hist_df[y1].max(), hist_df[y3].max())\n",
    "    offset = (y_max-y_min)/10.0\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Min: {best[y3].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best.index.to_numpy(), best[y3].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best.index.to_numpy(), best[y3].to_numpy()[0] + offset), # location of text\n",
    "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
    "\n",
    "    # Draw vertical line at best value\n",
    "    ax.axvline(x = best.index.to_numpy(), color = 'green', linestyle='-.', lw = 3);\n",
    "\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(y1.capitalize())\n",
    "    ax.set_title('Errors')\n",
    "    ax.grid();\n",
    "    ax.legend(loc = 'upper left') # model legend to upper left\n",
    "\n",
    "    ax = axes[1]\n",
    "\n",
    "    hist_df.plot( y = [y2, y4], ax = ax, colormap=CMAP)\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = \"Accuracy: \\n  train: {:6.4f}\\n  test:  {:6.4f}\"\n",
    "    txtstr = txtFmt.format(hist_df.iloc[-1][y2],\n",
    "                           hist_df.iloc[-1][y4]) #text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.2, txtstr, transform=ax.transAxes, fontsize=fontsize,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # calculate offset for arroe\n",
    "    y_min = min(hist_df[y2].min(), hist_df[y4].min())\n",
    "    y_max = max(hist_df[y2].max(), hist_df[y4].max())\n",
    "    offset = (y_max-y_min)/10.0\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Best: {best[y4].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best.index.to_numpy(), best[y4].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best.index.to_numpy(), best[y4].to_numpy()[0]-offset), # location of text\n",
    "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
    "\n",
    "\n",
    "    # Draw vertical line at best value\n",
    "    ax.axvline(x = best.index.to_numpy(), color = 'green', linestyle='-.', lw = 3);\n",
    "\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(y2.capitalize())\n",
    "    ax.grid()\n",
    "    ax.legend(loc = 'lower left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "###------------------------------\n",
    "### Plot Loss Curve using pytorch\n",
    "###------------------------------\n",
    "def fn_plot_torch_hist(hist_df):\n",
    "\n",
    "    # instantiate figure\n",
    "    fig, axes = plt.subplots(1,2 , figsize = (15,6))\n",
    "\n",
    "    # properties  matplotlib.patch.Patch\n",
    "    props = dict(boxstyle='round', facecolor='cyan', alpha=0.5)\n",
    "\n",
    "    # columns\n",
    "    x = hist_df.columns[0]\n",
    "    y1 = hist_df.columns[1]\n",
    "    y2 = hist_df.columns[2]\n",
    "    y3 = hist_df.columns[3]\n",
    "    y4 = hist_df.columns[4]\n",
    "\n",
    "    # Where was min loss\n",
    "    best = hist_df[hist_df[y2] == hist_df[y2].min()]\n",
    "    best = best.drop_duplicates(subset=y2)\n",
    "    # pick first axis\n",
    "    ax = axes[0]\n",
    "\n",
    "    # Plot all losses\n",
    "    hist_df.plot(x = x, y = [y1, y2], ax = ax)\n",
    "\n",
    "    # calculate offset for arroe\n",
    "    y_min = min(hist_df[y1].min(), hist_df[y2].min())\n",
    "    y_max = max(hist_df[y1].max(), hist_df[y2].max())\n",
    "    offset = (y_max-y_min)/10.0\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = \"Loss: \\n  train: {:6.4f}\\n   test: {:6.4f}\"\n",
    "    txtstr = txtFmt.format(hist_df.iloc[-1][y1],\n",
    "                           hist_df.iloc[-1][y2]) #text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.95, txtstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Min: {best[y2].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best[x].to_numpy(), best[y2].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best[x].to_numpy()+ offset, best[y2].to_numpy()[0]+offset), # location of text\n",
    "                fontsize=14,va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor='cyan', shrink=0.05)) # arrow\n",
    "\n",
    "    # Draw vertical line at best value\n",
    "    ax.axvline(x = best[x].to_numpy(), color = 'green', linestyle='-.', lw = 3);\n",
    "\n",
    "    ax.set_xlabel(x.title())\n",
    "    ax.set_ylabel(y1.title())\n",
    "    ax.set_title('Errors')\n",
    "    ax.grid()\n",
    "    ax.legend(loc = 'upper left') # model legend to upper left\n",
    "\n",
    "    # pick second axis\n",
    "    ax = axes[1]\n",
    "\n",
    "    # Plot accuracies\n",
    "    hist_df.plot(x = x, y = [y3, y4], ax = ax)\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = \"Accuracy: \\n  train: {:6.4f}\\n  test:  {:6.4f}\"\n",
    "    txtstr = txtFmt.format(hist_df.iloc[-1][y3],\n",
    "                           hist_df.iloc[-1][y4]) #text to plot\n",
    "\n",
    "    # calculate offset for arroe\n",
    "    y_min = min(hist_df[y3].min(), hist_df[y4].min())\n",
    "    y_max = max(hist_df[y3].max(), hist_df[y4].max())\n",
    "    offset = (y_max-y_min)/10.0\n",
    "\n",
    "    # place a text box in lower middle in axes coords\n",
    "    ax.text(0.3, 0.2, txtstr, transform=ax.transAxes, fontsize=12,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Best: {best[y4].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best[x].to_numpy(), best[y4].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best[x].to_numpy()- offset, best[y4].to_numpy()[0]-offset), # location of text\n",
    "                fontsize=14,va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor='cyan', shrink=0.05)) # arrow\n",
    "\n",
    "\n",
    "    # Draw a vertical line at best value\n",
    "    ax.axvline(x = best[x].to_numpy(),\n",
    "               color = 'green',\n",
    "               linestyle='-.', lw = 3)\n",
    "\n",
    "    # Labels\n",
    "    ax.set_xlabel(x.title())\n",
    "    ax.set_ylabel(y3.title())\n",
    "    ax.set_title('Accuracies')\n",
    "    ax.grid();\n",
    "    ax.legend(loc = 'lower left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "###---------------------------------------------------\n",
    "### Plot count of classes in training and testing sets\n",
    "###---------------------------------------------------\n",
    "def fn_plot_labels(tr_ds, ts_ds, class_names = None):\n",
    "\n",
    "    '''\n",
    "        Args:\n",
    "            tr_ds :  Training Dataset\n",
    "            ts_ds : Testing dataset\n",
    "        Returns : none\n",
    "    '''\n",
    "\n",
    "    # create figure and axes\n",
    "    fig, axes = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "    # get names of the classes\n",
    "    if not class_names:\n",
    "        tr_class_names = tr_ds.class_names\n",
    "        ts_class_names = tr_ds.class_names\n",
    "\n",
    "    # pick first axis\n",
    "    ax = axes[0]\n",
    "\n",
    "    # create dict of training labels\n",
    "    class_counts = {}\n",
    "    for imgs, lbls in tr_ds:\n",
    "        for lbl in lbls.numpy():\n",
    "            class_counts[lbl] = class_counts.get(lbl, 0) +1\n",
    "\n",
    "    # bar plot\n",
    "    ax.bar(tr_class_names, [class_counts.get(i, 0) for i in range(len(tr_class_names))],\n",
    "           align='center',color = 'DarkBlue', alpha = 0.7)\n",
    "\n",
    "    # add title\n",
    "    ax.set_title('Training Set')\n",
    "\n",
    "    # grids make it look good\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "    #pick second image\n",
    "    ax = axes[1]\n",
    "\n",
    "    # create dict of training labels\n",
    "    class_counts = {}\n",
    "    for imgs, lbls in ts_ds:\n",
    "        for lbl in lbls.numpy():\n",
    "            class_counts[lbl] = class_counts.get(lbl, 0) +1\n",
    "\n",
    "    # bar plot\n",
    "    ax.bar(ts_class_names, [class_counts.get(i, 0) for i in range(len(ts_class_names))],\n",
    "           align='center',color = 'orange', alpha = 0.7)\n",
    "\n",
    "    # add title\n",
    "    ax.set_title('Test Set')\n",
    "\n",
    "\n",
    "    # grids make it look good\n",
    "    ax.grid()\n",
    "\n",
    "    # fit the subplot(s) in to the figure area\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # display all open figures\n",
    "    plt.show()\n",
    "\n",
    "def fn_convert_timestamp(tstr):\n",
    "    '''\n",
    "        Function to conver string of form \"2015-11-12 1444\"\n",
    "    '''\n",
    "\n",
    "    return datetime.strptime(tstr, \"%Y-%m-%d_%H%M\")\n",
    "\n",
    "###----------------------\n",
    "### Plot confusion matrix\n",
    "###----------------------\n",
    "def fn_plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    '''\n",
    "    Args:\n",
    "        y_true: Ground Truth\n",
    "        y_pred : Predictions\n",
    "        labels : dictonary\n",
    "                  {0: 'Goal Keeper',\n",
    "                  1: 'Defender',\n",
    "                  2: 'Mid-Fielder',\n",
    "                  3: 'Forward'}\n",
    "\n",
    "    '''\n",
    "\n",
    "    cm  = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                  display_labels=labels.values())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (6,6))\n",
    "\n",
    "    disp.plot(ax = ax, cmap = 'Blues', xticks_rotation = 'vertical', colorbar=False)\n",
    "    # Disable the grid\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Hygiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iAve6DCL4JH4"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print (physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nlabels = []\\n\\nfor count, data in enumerate (train_ds):\\n    imgs, lbls = data\\n    #print(count, \":\", imgs.shape, lbls.shape)\\n    labels.append(lbls)\\n    \\ntf.concat(labels, axis = 0).numpy().shape\\n\\nunique, A, counts = tf.unique_with_counts(tf.concat(labels, axis = 0).numpy())\\nunique, A, counts\\n\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "\n",
    "labels = []\n",
    "\n",
    "for count, data in enumerate (train_ds):\n",
    "    imgs, lbls = data\n",
    "    #print(count, \":\", imgs.shape, lbls.shape)\n",
    "    labels.append(lbls)\n",
    "    \n",
    "tf.concat(labels, axis = 0).numpy().shape\n",
    "\n",
    "unique, A, counts = tf.unique_with_counts(tf.concat(labels, axis = 0).numpy())\n",
    "unique, A, counts\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_label(tr_ds, ts_ds):\n",
    "    \n",
    "    plt.figure(figsize = (15,5)) # instantiate the figure\n",
    "    \n",
    "    plt.subplot(1,2,1) # first out of 2\n",
    "\n",
    "    train_labels = tf.concat([lbl for img, lbl in tr_ds], axis = 0).numpy() # get the labels\n",
    "\n",
    "    unique, _, counts = tf.unique_with_counts(train_labels) # get counts\n",
    "\n",
    "    plt.bar(range(len(unique)), counts, align='center', color = 'DarkBlue') # barplot the counts\n",
    "\n",
    "    plt.xticks(range(len(unique)), class_names)\n",
    "\n",
    "    plt.title('Training Set')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    \n",
    "    test_labels = tf.concat([lbl for img, lbl in ts_ds], axis = 0).numpy()\n",
    "\n",
    "    unique, _, counts = tf.unique_with_counts(test_labels)\n",
    "\n",
    "    plt.bar(range(len(unique)), counts, align='center', color = 'Orange')\n",
    "\n",
    "    plt.xticks(range(len(unique)), class_names)\n",
    "\n",
    "    plt.title('Test Set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pathlib\\ndataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\\n\\ndata_dir = tf.keras.utils.get_file(origin=dataset_url,\\n                                   fname=\\'flower_photos\\',\\n                                   untar=True)\\ndata_dir = pathlib.Path(data_dir)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comment part is for colab\n",
    "\n",
    "'''import pathlib\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "\n",
    "data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "                                   fname='flower_photos',\n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)'''\n",
    "\n",
    "\n",
    "# data_dir = os.path.join(inpDir, subDir)\n",
    "# data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory C:/Users/Rohit/OneDrive/Desktop/DNN_Assign/input\\flower_photos",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# create training data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m train_ds \u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m      4\u001b[0m     \n\u001b[0;32m      5\u001b[0m     data_dir, \u001b[38;5;66;03m# path the the data directory\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \n\u001b[0;32m      7\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39mTEST_SIZE, \u001b[38;5;66;03m# what ratio of validation data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \n\u001b[0;32m      9\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# purpose\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \n\u001b[0;32m     11\u001b[0m     seed\u001b[38;5;241m=\u001b[39mRANDOM_STATE, \n\u001b[0;32m     12\u001b[0m     \n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# what size of image should we bring?\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m[IMG_HEIGHT, IMG_WIDTH], \u001b[38;5;66;03m## @@@ WHAT!\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# 80 % of image to the training set\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# test data\u001b[39;00m\n\u001b[0;32m     21\u001b[0m test_ds \u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m     22\u001b[0m     \n\u001b[0;32m     23\u001b[0m     data_dir, \u001b[38;5;66;03m# path the the data directory\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\image_dataset_utils.py:223\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[1;32m--> 223\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mindex_directory(\n\u001b[0;32m    224\u001b[0m     directory,\n\u001b[0;32m    225\u001b[0m     labels,\n\u001b[0;32m    226\u001b[0m     formats\u001b[38;5;241m=\u001b[39mALLOWLIST_FORMATS,\n\u001b[0;32m    227\u001b[0m     class_names\u001b[38;5;241m=\u001b[39mclass_names,\n\u001b[0;32m    228\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m    229\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    230\u001b[0m     follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[0;32m    231\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    232\u001b[0m )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:539\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    541\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[1;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[0;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    778\u001b[0m ]\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find directory C:/Users/Rohit/OneDrive/Desktop/DNN_Assign/input\\flower_photos"
     ]
    }
   ],
   "source": [
    "# create training data\n",
    "\n",
    "train_ds =tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \n",
    "    data_dir, # path the the data directory\n",
    "    \n",
    "    validation_split=TEST_SIZE, # what ratio of validation data\n",
    "    \n",
    "    subset='training', # purpose\n",
    "    \n",
    "    seed=RANDOM_STATE, \n",
    "    \n",
    "    # what size of image should we bring?\n",
    "    image_size=[IMG_HEIGHT, IMG_WIDTH], ## @@@ WHAT!\n",
    "    \n",
    "    # 80 % of image to the training set\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "# test data\n",
    "\n",
    "test_ds =tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \n",
    "    data_dir, # path the the data directory\n",
    "    \n",
    "    validation_split=TEST_SIZE, # what ratio of validation data\n",
    "    \n",
    "    subset='validation', # purpose\n",
    "    \n",
    "    seed=RANDOM_STATE, \n",
    "    \n",
    "    image_size=[IMG_HEIGHT, IMG_WIDTH],\n",
    "    \n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# is it picking class names\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m class_names \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mclass_names\n\u001b[0;32m      4\u001b[0m class_names\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# is it picking class names\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {k:v for k,v in enumerate(class_names)}\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range (BATCH_SIZE):\n",
    "        plt.subplot(int(BATCH_SIZE/8), 8, i +1)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "for images, labels in test_ds.take(1): # get me one batch\n",
    "    \n",
    "    for i in range (BATCH_SIZE): # loop over batch\n",
    "        \n",
    "        plt.subplot(int(BATCH_SIZE/8), 8, i +1) # access the axis\n",
    "        \n",
    "        plt.grid(False) # no to grid\n",
    "        \n",
    "        plt.imshow(images[i].numpy().astype('uint8')) # show image convert to numpy and int\n",
    "        \n",
    "        plt.title(class_names[labels[i]])\n",
    "        \n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_plot_label(train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds=test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "densenet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model=tf.keras.applications.DenseNet169(weights='imagenet',\n",
    "                                                 include_top=False,\n",
    "                                                input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in densenet_model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    \n",
    "    krnl_init = tf.keras.initializers.GlorotUniform(seed = RANDOM_STATE)\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Rescaling (1./255.))\n",
    "\n",
    "    # #augmentation layer here\n",
    "    # model.add(tf.keras.layers.RandomFlip(mode=FLIP_MODE,         \n",
    "    #                             kernel_initializer= krnl_init,\n",
    "    #                             activation='relu'))       # expected output size  = 92 x 92 x 64\n",
    "\n",
    "\n",
    "    model.add(densenet_model)\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "#first\n",
    "    model.add(tf.keras.layers.Dense(1664,kernel_initializer= krnl_init))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    \n",
    "#second\n",
    "    model.add(tf.keras.layers.Dense(46,kernel_initializer= krnl_init))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    \n",
    "#output\n",
    "    model.add(tf.keras.layers.Dense(num_classes,kernel_initializer= krnl_init))\n",
    "   \n",
    "    return model                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(input_shape, num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss:\n",
    "\n",
    "loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=loss_fn,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(train_ds,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=EPOCHS,\n",
    "                  validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tf.keras.utils.plot_model(model,\"model.png\",\n",
    "                          show_shapes=True,\n",
    "                          show_dtype=True,\n",
    "                          dpi=96,\n",
    "                          show_layer_activations=True )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_plot_tf_hist(pd.DataFrame(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=tf.keras.layers.RandomRotation((-0.5,0.5),\n",
    "    fill_mode=\"nearest\",\n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "img_num=0\n",
    "\n",
    "for images,labels in train_ds.take(1):\n",
    "    out_images=layer(images)\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"original\")\n",
    "    plt.imshow(images[img_num].numpy().astype(\"uint16\"))\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"rotated\")\n",
    "    plt.imshow(out_images[img_num].numpy().astype(\"uint16\"))\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc = model.evaluate(test_ds, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(test_ds)\n",
    "\n",
    "y_test = np.concatenate([y for X, y in test_ds], axis = 0).squeeze()\n",
    "\n",
    "y_pred = yhat.argmax(axis = 1)\n",
    "\n",
    "print('Accuracy score on test data: , {:.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running on validation data:\n",
    "\n",
    "val_dir = os.path.join(inpDir, subDir)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        val_dir,            # path the the data directory\n",
    "        validation_split=None,   # what ratio of validation data\n",
    "        seed=RANDOM_STATE, \n",
    "        image_size=[IMG_HEIGHT, IMG_WIDTH], \n",
    "        batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(val_ds)\n",
    "\n",
    "y_test = np.concatenate([y for X, y in val_ds], axis = 0).squeeze()\n",
    "\n",
    "y_pred = yhat.argmax(axis = 1)\n",
    "\n",
    "print('Accuracy score on test data: , {:.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
